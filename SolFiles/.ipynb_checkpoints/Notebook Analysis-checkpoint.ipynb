{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First of all install these must dependencies to run the program\n",
    "### FFMPEG tool for transcoding multimedia files:\n",
    "* For Ubuntu: https://linuxize.com/post/how-to-install-ffmpeg-on-ubuntu-18-04/\n",
    "* For Windows: https://www.wikihow.com/Install-FFmpeg-on-Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install imutils\n",
    "#! pip install tqdm\n",
    "#! pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import cv2\n",
    "import subprocess\n",
    "import re\n",
    "import math\n",
    "from subprocess import check_call, PIPE, Popen\n",
    "import shlex\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we can see we are using opencv library for python which is a very important open source video analysis library avaialble*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this we will use several pre-trained models or haar cascade files( It is ML object detection algorithms to determine objects in an image* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_upper_body_cascade = cv2.CascadeClassifier(\"models/haarcascade_upperbody.xml\")\n",
    "haar_full_body_cascade = cv2.CascadeClassifier(\"models/haarcascade_fullbody.xml\")\n",
    "haar_face_cascade = cv2.CascadeClassifier('models/haarcascade_frontalface_alt.xml')\n",
    "lbm_face_cascade = cv2.CascadeClassifier('models/lbpcascade_frontalface.xml')\n",
    "haar_hand_cascade = cv2.CascadeClassifier('models/hand.xml')\n",
    "haar_lowerbody_cascade = cv2.CascadeClassifier('model/haarcascade_lowerbody.xml')\n",
    "text_detection_model = \"models/frozen_east_text_detection.pb\"\n",
    "\n",
    "#text_detection_model = \"https://raw.githubusercontent.com/oyyd/ frozen_east_text_detection.pb/master/frozen_east_text_detection.pb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Loading different files where we will store our keyframes, segments and result about the video summarization *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../dataset'\n",
    "SEGMENT_OUTPUT = '../segments'\n",
    "OUTPUT_FOLDER = 'results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is use to get what is the frame we are getting in video segments, P-frame or I-frame. Genrally I-frame have all the independent features to represent the image data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frame_types(filename):\n",
    "    cmd = 'ffprobe -v error -show_entries frame=pict_type -of default=noprint_wrappers=1'.split()\n",
    "    out = subprocess.check_output(cmd + [filename]).decode()\n",
    "    frame_types = out.replace('pict_type=', '').split()\n",
    "    return zip(range(len(frame_types)), frame_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It calculates the video length in seconds*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_length(video_fn):\n",
    "    res = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
    "                             \"format=duration\", \"-of\",\n",
    "                             \"default=noprint_wrappers=1:nokey=1\", video_fn],\n",
    "                            stdout=subprocess.PIPE,\n",
    "                            stderr=subprocess.STDOUT)\n",
    "    return float(res.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vipul/Desktop/AI_ML_Capstone_Mar/Capstone Project 1/Solution Files'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4160277833333335"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_video_length(\"../dataset/nptel_ml/ML.mp4\")/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This function will split the data into number of segments divided by seconds we specify in our case 30 sec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_metadata = re.compile('Duration: (\\d{2}):(\\d{2}):(\\d{2})\\.\\d+,.*\\n.* (\\d+(\\.\\d+)?) fps')\n",
    "\n",
    "\n",
    "def get_video_metadata(video_fn):\n",
    "    p1 = Popen([\"ffmpeg\", \"-hide_banner\", \"-i\", video_fn], stderr=PIPE, universal_newlines=True)\n",
    "    output = p1.communicate()[1]\n",
    "    matches = re_metadata.search(output)\n",
    "    if matches:\n",
    "        video_length = int(matches.group(1)) * 3600 + int(matches.group(2)) * 60 + int(matches.group(3))\n",
    "        video_fps = float(matches.group(4))\n",
    "    else:\n",
    "        raise Exception(\"Can't parse required video metadata\")\n",
    "    return video_length, video_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video_in_segment(video_fn, num, out_path, by='size'):\n",
    "    assert num > 0\n",
    "    assert by in ['size', 'count']\n",
    "    split_video_size = num if by == 'size' else None\n",
    "    split_video_count = num if by == 'count' else None\n",
    "    # parsing meta-data\n",
    "    video_length, video_fps = get_video_metadata(video_fn)\n",
    "\n",
    "    # calculate the video video split count\n",
    "    if split_video_size:\n",
    "        split_video_count = math.ceil(video_length / split_video_size)\n",
    "        if split_video_count == 1:\n",
    "            raise Exception(\"Too small split size! Please increase the target split size!!\")\n",
    "    else: # split video count\n",
    "        split_video_size = round(video_length / split_video_count)\n",
    "    \n",
    "    # For windows\n",
    "    #pth, ext = video_fn.rsplit(\".\", 1)\n",
    "    #print(\"pth: {},ext: {}\".format(pth,ext))\n",
    "    #temp_pth = pth.rsplit(\"\\\\\",1)\n",
    "    #print(\"temp_pth: {}\".format(temp_pth))\n",
    "    #folder_name = temp_pth[0].rsplit(\"\\\\\",1)\n",
    "    #print(\"folder_name: {}\".format(folder_name))\n",
    "    #pth = out_path + os.sep + folder_name[1] + os.sep + temp_pth[1]\n",
    "    #print(\"updated path: {},ext: {}\".format(pth,ext))\n",
    "    \n",
    "    # For Linux Machines\n",
    "    pth, ext = video_fn.rsplit(\".\", 1)\n",
    "    temp_pth = pth.rsplit(\"/\",1)\n",
    "    folder_name = temp_pth[0].rsplit(\"/\",1)\n",
    "    pth = out_path + os.sep + folder_name[1] + os.sep + temp_pth[1]\n",
    "    #cmd = 'ffmpeg -hide_banner -loglevel panic -i \"{}\" -c copy -map 0 -segment_time {} -reset_timestamps 1 -g {} -sc_threshold 0 -force_key_frames \"expr:gte(t,n_forced*{})\" -f segment -y \"{}-%d.{}\"'.format(video_fn, split_video_size, round(split_video_size*video_fps), split_video_size, pth, ext)\n",
    "    cmd = 'ffmpeg -i \"{}\" -c copy -map 0 -f segment -segment_time 60 -reset_timestamps 1 -y \"{}-%d.{}\"'.format(video_fn, pth, ext)\n",
    "    check_call(shlex.split(cmd), universal_newlines=True)\n",
    "\n",
    "    # returning the list of output (index start from 0)\n",
    "    return ['{}-{}.{}'.format(pth, i, ext) for i in range(split_video_count)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Video Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This loop will iterate over the folders and the videos inside it and call the split function, the split will be saved in segments folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ../dataset/nptel_ai/How to Learn and Follow the Course.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.78it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.59it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00, 12.30it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../segments/nptel_ai/How to Learn and Follow the Course-0.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-1.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-2.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-3.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-4.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-5.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-6.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-7.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-8.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-9.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-10.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-11.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-12.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-13.mp4', '../segments/nptel_ai/How to Learn and Follow the Course-14.mp4']\n",
      "Path: ../dataset/nptel_ml/ML.mp4\n",
      "['../segments/nptel_ml/ML-0.mp4', '../segments/nptel_ml/ML-1.mp4', '../segments/nptel_ml/ML-2.mp4', '../segments/nptel_ml/ML-3.mp4', '../segments/nptel_ml/ML-4.mp4', '../segments/nptel_ml/ML-5.mp4', '../segments/nptel_ml/ML-6.mp4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(os.listdir(DATA_FOLDER)):\n",
    "    for f in tqdm(os.listdir(DATA_FOLDER+os.sep+file)):\n",
    "        if f.endswith((\".mp4\", \".avi\", \".mov\", \".wmv\", \".qt\", \".MTS\", \".M2TS\", \".TS\", \".mkv\", \".flv\", \".vob\")):\n",
    "            path=os.path.join(DATA_FOLDER,file,f)\n",
    "            print(\"Path: {}\".format(path))\n",
    "            print(split_video_in_segment(path, 30,SEGMENT_OUTPUT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is the for loop which get all the segments in different files, and get all the i-frames and classify and cluster the image based upon the face,hand,upper_body,text and Area of text occupied in image and store the data in csv file in result*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follwing Task in this block of cell and code\n",
    "* Task 2: Assessment of Instructor Presence and Interaction\n",
    "* Task 3: Assessment of use of blackboard, slides\n",
    "\n",
    "The result of the output saved in Result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading text detector...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-iwig8vc6/opencv/modules/dnn/src/caffe/caffe_io.cpp:1133: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"https://raw.githubusercontent.com/oyyd/ frozen_east_text_detection.pb/master/frozen_east_text_detection.pb\" in function 'ReadProtoFromBinaryFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0ecfe6db620e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the pre-trained text detecter to detect text in the frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] loading text detector...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_detection_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseg_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEGMENT_OUTPUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Initialize the parameters for result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-iwig8vc6/opencv/modules/dnn/src/caffe/caffe_io.cpp:1133: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"https://raw.githubusercontent.com/oyyd/ frozen_east_text_detection.pb/master/frozen_east_text_detection.pb\" in function 'ReadProtoFromBinaryFile'\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained text detecter to detect text in the frame\n",
    "print(\"[INFO] loading text detector...\")\n",
    "net = cv2.dnn.readNet(text_detection_model)\n",
    "for seg_file in tqdm(os.listdir(SEGMENT_OUTPUT)):\n",
    "    # Initialize the parameters for result\n",
    "    params = {\"Keyframe_number\": [], \"Instructor_detected\": [], \"Hand_detected\": [],\n",
    "                \"Instructor_upperBody_detected\": [],\n",
    "                \"Instructor_fullBody_detected\": [], \"Teaching_method\": [],\n",
    "                \"Text_detected\": [], \"Percent_of_area_occupied_by_text\": [], \"Too_much_text_occupied\": []}\n",
    "    video_found = False\n",
    "    total_video_len = 0\n",
    "    total_segments = 0\n",
    "    #print(\"Taking segments.....\")\n",
    "    for video_fn in os.listdir(SEGMENT_OUTPUT + os.sep + seg_file):\n",
    "        # loop through the files with video extensions only \n",
    "        if video_fn.endswith(\n",
    "                (\".mp4\", \".avi\", \".mov\", \".wmv\", \".qt\", \".MTS\", \".M2TS\", \".TS\", \".mkv\", \".flv\", \".vob\")):\n",
    "            video_found = True\n",
    "            path = os.path.join(SEGMENT_OUTPUT, seg_file, video_fn)\n",
    "            temp_video_len = get_video_length(path)\n",
    "            total_video_len += temp_video_len\n",
    "            total_segments += 1\n",
    "            frame_types = get_video_frame_types(SEGMENT_OUTPUT + os.sep + seg_file + os.sep + video_fn)\n",
    "            i_frames = [x[0] for x in frame_types if x[1] == 'I']\n",
    "            # once I frame is got , we can loop in I frames to detect features\n",
    "            if i_frames:\n",
    "                basename = os.path.splitext(os.path.basename(video_fn))[0]\n",
    "                cap = cv2.VideoCapture(SEGMENT_OUTPUT + os.sep + seg_file + os.sep + video_fn)\n",
    "\n",
    "                keyframe_number = \"\"\n",
    "                Instructor_detected = False\n",
    "                Hand_detected = False\n",
    "                Instructor_upperBody_detected = False\n",
    "                Instructor_fullBody_detected = False\n",
    "                Teaching_method = \"No board used\"\n",
    "                Text_detected = False\n",
    "                Percent_of_area_occupied_by_text = 0\n",
    "                Too_much_text_occupied = False\n",
    "\n",
    "                for frame_no in i_frames:\n",
    "\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
    "                    ret, frame = cap.read()\n",
    "                    keyframe_index = basename + '_i_frame_' + str(frame_no)\n",
    "                    keyframe_number = keyframe_index\n",
    "                    outname = keyframe_index + '.jpg'\n",
    "                    cv2.imwrite('keyframes/' + outname, frame)\n",
    "                    # print('Saved: ' + outname)\n",
    "                    # converts to gray\n",
    "                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    # Checking which board instructor is using slides, greenBoard, blackboard\n",
    "                    hsv_image = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "                    reshape = hsv_image.reshape((hsv_image.shape[0] * hsv_image.shape[1], 3))\n",
    "\n",
    "                    # Find and display most dominant colors\n",
    "                    cluster = KMeans(n_clusters=3).fit(reshape)\n",
    "                    centroids = cluster.cluster_centers_\n",
    "                    # Get the number of different clusters, create histogram, and normalize\n",
    "                    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
    "                    (hist, _) = np.histogram(cluster.labels_, bins=labels)\n",
    "                    hist = hist.astype(\"float\")\n",
    "                    hist /= hist.sum()\n",
    "\n",
    "                    # It will create the histogram and iterate into every colors\n",
    "                    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)], reverse=True)\n",
    "\n",
    "                    if 36 < colors[0][1][0] < 86:\n",
    "                        if 25 < colors[0][1][1] < 255:\n",
    "                            if 25 < colors[0][1][2] < 255:\n",
    "                                Teaching_method = \"GreenBoard\"\n",
    "\n",
    "                    if 0 < colors[0][1][0] < 179:\n",
    "                        if 5 < colors[0][1][1] < 50:\n",
    "                            if 50 < colors[0][1][2] < 255:\n",
    "                                Teaching_method = \"BlackBoard\"\n",
    "\n",
    "                    if 0 < colors[0][1][0] < 172:\n",
    "                        if 0 < colors[0][1][1] < 111:\n",
    "                            if 168 < colors[0][1][2] < 255:\n",
    "                                Teaching_method = \"slide\"\n",
    "                    # Detect the face\n",
    "                    #print(\"Now detecting face...\")\n",
    "                    faces = lbm_face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5,\n",
    "                                                                minSize=(50, 100),\n",
    "                                                                # Min size for valid detection, changes according\n",
    "                                                                # to video size or body size in the video.\n",
    "                                                                flags=cv2.CASCADE_SCALE_IMAGE\n",
    "                                                                )\n",
    "                    # Upper body detection\n",
    "                    #print(\"Upper body Detection...\")\n",
    "                    upper_body = haar_upper_body_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5,\n",
    "                                                                            minSize=(50, 100),\n",
    "                                                                            # Min size for valid detection, changes\n",
    "                                                                            # according to video size or body size\n",
    "                                                                            # in the video.\n",
    "                                                                            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "                                                                            )\n",
    "                    # Full body detection\n",
    "                    #print(\"Full Body Detection...\")\n",
    "                    full_body = haar_full_body_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5,\n",
    "                                                                        minSize=(50, 100),\n",
    "                                                                        # Min size for valid detection, changes\n",
    "                                                                        # according to video size or body size in\n",
    "                                                                        # the video.\n",
    "                                                                        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "                                                                        )\n",
    "\n",
    "                    # Hand detection\n",
    "                    #print(\"Hand detection....\")\n",
    "                    hands = haar_hand_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5,\n",
    "                                                                minSize=(50, 100),\n",
    "                                                                # Min size for valid detection, changes according\n",
    "                                                                # to video size or body size in the video.\n",
    "                                                                flags=cv2.CASCADE_SCALE_IMAGE\n",
    "                                                                )\n",
    "                    # Getting detection points\n",
    "\n",
    "                    #for (x, y, w, h) in faces:\n",
    "                    if len(faces)>0:\n",
    "                        Instructor_detected = True\n",
    "                    #for (x, y, w, h) in upper_body:\n",
    "                    if len(upper_body)>0:\n",
    "                        Instructor_upperBody_detected = True\n",
    "                    #for (x, y, w, h) in full_body:\n",
    "                    if len(full_body)>0:\n",
    "                        Instructor_fullBody_detected = True\n",
    "                    #for (x, y, w, h) in hands:\n",
    "                    if len(hands)>0:\n",
    "                        Hand_detected = True\n",
    "                    # Text detection\n",
    "                    # Detecting the dimention of image\n",
    "                    image = frame.copy()\n",
    "                    (H, W) = image.shape[:2]\n",
    "\n",
    "                    # taking the ratio of new dimention set\n",
    "                    # for both the width and height\n",
    "                    (newW, newH) = (320, 320)\n",
    "                    rW = W / float(newW)\n",
    "                    rH = H / float(newH)\n",
    "\n",
    "                    # taking new dimention image after resizing\n",
    "                    image = cv2.resize(image, (newW, newH))\n",
    "                    (H, W) = image.shape[:2]\n",
    "\n",
    "                    # define two output layer for model\n",
    "                    layerNames = [\n",
    "                        \"feature_fusion/Conv_7/Sigmoid\",\n",
    "                        \"feature_fusion/concat_3\"]\n",
    "                    # forward pass\n",
    "                    # to obtain two output layer of model\n",
    "                    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "                                                    (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "\n",
    "                    start = time.time()\n",
    "                    net.setInput(blob)\n",
    "                    (scores, geometry) = net.forward(layerNames)\n",
    "                    end = time.time()\n",
    "\n",
    "                    # show timing information on text prediction\n",
    "                    print(\"[INFO] text detection took {:.6f} seconds\".format(end - start))\n",
    "\n",
    "                    # scores value row and column\n",
    "                    # bounding boxes initialization\n",
    "                    # confidence scores\n",
    "                    (numRows, numCols) = scores.shape[2:4]\n",
    "                    rects = []\n",
    "                    confidences = []\n",
    "\n",
    "                    # loop over the number of rows\n",
    "                    for y in range(0, numRows):\n",
    "                        # extract the scores (probabilities), followed by the geometrical\n",
    "                        # to detect the bounding box of the data\n",
    "                        # surround text\n",
    "                        scoresData = scores[0, 0, y]\n",
    "                        xData0 = geometry[0, 0, y]\n",
    "                        xData1 = geometry[0, 1, y]\n",
    "                        xData2 = geometry[0, 2, y]\n",
    "                        xData3 = geometry[0, 3, y]\n",
    "                        anglesData = geometry[0, 4, y]\n",
    "\n",
    "                        # loop over the number of columns\n",
    "                        for x in range(0, numCols):\n",
    "                            # ignore if the scores is not enough\n",
    "                            if scoresData[x] < 0.5:\n",
    "                                continue\n",
    "\n",
    "                            # computing our feature maps\n",
    "                            # be 4x smaller than the input image\n",
    "                            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "\n",
    "                            # rotation angle for prediction\n",
    "                            # compute the sin and cosine\n",
    "                            angle = anglesData[x]\n",
    "                            cos = np.cos(angle)\n",
    "                            sin = np.sin(angle)\n",
    "\n",
    "                            # getting width and height\n",
    "                            # the bounding box\n",
    "                            h = xData0[x] + xData2[x]\n",
    "                            w = xData1[x] + xData3[x]\n",
    "\n",
    "                            # x and y staring and ending coordinates\n",
    "                            # the text prediction bounding box\n",
    "                            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "                            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "                            startX = int(endX - w)\n",
    "                            startY = int(endY - h)\n",
    "\n",
    "                            # add the bounding box coordinates and probability score to\n",
    "                            # our respective lists\n",
    "                            rects.append((startX, startY, endX, endY))\n",
    "                            confidences.append(scoresData[x])\n",
    "\n",
    "                    # suppress weak and overlapping bounding\n",
    "                    # boxes\n",
    "                    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "\n",
    "                    # loop over the bounding boxes\n",
    "                    area_temp = []\n",
    "                    for (startX, startY, endX, endY) in boxes:\n",
    "                        # scale the box coordinates\n",
    "                        # ratios\n",
    "                        startX = int(startX * rW)\n",
    "                        startY = int(startY * rH)\n",
    "                        endX = int(endX * rW)\n",
    "                        endY = int(endY * rH)\n",
    "                        Text_detected = True\n",
    "                        # draw the bounding box on the image\n",
    "                        area_temp.append(((abs(endX - startX) * abs(endY - startY)) / (H * W)) * 100)\n",
    "                    Percent_of_area_occupied_by_text = sum(area_temp)\n",
    "                    # get all the parameters\n",
    "                    params[\"Keyframe_number\"].append(keyframe_number)\n",
    "                    params[\"Instructor_detected\"].append(Instructor_detected)\n",
    "                    params[\"Hand_detected\"].append(Hand_detected)\n",
    "                    params[\"Instructor_upperBody_detected\"].append(Instructor_upperBody_detected)\n",
    "                    params[\"Instructor_fullBody_detected\"].append(Instructor_fullBody_detected)\n",
    "                    params[\"Teaching_method\"].append(Teaching_method)\n",
    "                    params[\"Text_detected\"].append(Text_detected)\n",
    "                    params[\"Percent_of_area_occupied_by_text\"].append(Percent_of_area_occupied_by_text)\n",
    "                    if Percent_of_area_occupied_by_text > 60:\n",
    "                        params[\"Too_much_text_occupied\"].append(\"Too Much Text detected\")\n",
    "                    else:\n",
    "                        params[\"Too_much_text_occupied\"].append(Too_much_text_occupied)\n",
    "\n",
    "                cap.release()\n",
    "            else:\n",
    "                print('No I-frames in ' + video_fn)\n",
    "    if video_found:\n",
    "        data = pd.DataFrame(params)\n",
    "        data.set_index(\"Keyframe_number\")\n",
    "        data.to_csv(OUTPUT_FOLDER + os.sep + seg_file + \"video_details\" +\".csv\", index=False)\n",
    "        # average details\n",
    "        Instructor_presence = \"\"\n",
    "        if len(data.Instructor_detected.unique()) == 2:\n",
    "            Instructor_presence = \"Part of the video frame\"\n",
    "        elif list(data.Instructor_detected.unique())[0]:\n",
    "            Instructor_presence = \"Whole screen\"\n",
    "        else:\n",
    "            Instructor_presence = \"No Instructor detected\"\n",
    "\n",
    "        Interaction_instructor = []\n",
    "        if len(data.Hand_detected.unique()) == 2:\n",
    "            Interaction_instructor.append(\"hand\")\n",
    "        elif list(data.Hand_detected.unique())[0]:\n",
    "            Interaction_instructor.append(\"Hand\")\n",
    "\n",
    "        if len(data.Instructor_upperBody_detected.unique()) == 2:\n",
    "            Interaction_instructor.append(\"UpperBody\")\n",
    "        elif list(data.Instructor_upperBody_detected.unique())[0]:\n",
    "            Interaction_instructor.append(\"UpperBody\")\n",
    "\n",
    "        if len(data.Instructor_fullBody_detected.unique()) == 2:\n",
    "            Interaction_instructor.append(\"FullBody\")\n",
    "        elif list(data.Instructor_fullBody_detected.unique())[0]:\n",
    "            Interaction_instructor.append(\"FullBody\")\n",
    "\n",
    "        avg_params = {\n",
    "            \"Total_I_KeyFrames\": [len(data.index)],\n",
    "            \"Total_segments_obtained\": [total_segments],\n",
    "            \"Video_length\": [str(total_video_len/60) + ' mins'],\n",
    "            \"Instructor_presence\": [Instructor_presence],\n",
    "            \"Instructor_gesture_interaction\": [Interaction_instructor],\n",
    "            \"Instructor_teaching_methods\": [list(data.Teaching_method.unique())[:-1] if len(data.Teaching_method.unique()) > 1 else list(data.Teaching_method.unique())[0]]\n",
    "        }\n",
    "        df = pd.DataFrame(avg_params)\n",
    "        df.to_csv(OUTPUT_FOLDER + os.sep + seg_file + \"video_average_details\" + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
